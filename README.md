# RepresentaÃ§Ãµes Latentes com Autoencoders

Os **Autoencoders** sÃ£o redes neurais artificiais utilizadas para **aprender representaÃ§Ãµes compactas e eficientes dos dados** (codificaÃ§Ã£o), com o objetivo de reconstruir a entrada original o mais fielmente possÃ­vel.

Eles sÃ£o amplamente usados para **compressÃ£o, remoÃ§Ã£o de ruÃ­do, prÃ©-treinamento e geraÃ§Ã£o de dados sintÃ©ticos**, funcionando como uma espÃ©cie de filtro de informaÃ§Ã£o.

### Por que Autoencoders sÃ£o Ãºteis?

- Aprendem **representaÃ§Ãµes latentes** dos dados de forma nÃ£o supervisionada
- Permitem **reduÃ§Ã£o de dimensionalidade** preservando informaÃ§Ãµes relevantes
- SÃ£o base para modelos geradores como **Variational Autoencoders (VAEs)**
- AplicÃ¡veis em **reconstruÃ§Ã£o de imagens, remoÃ§Ã£o de ruÃ­do e detecÃ§Ã£o de anomalias**

### Funcionamento Intuitivo

Pense em um **autoencoder como um tradutor eficiente**: ele pega uma imagem de entrada, "traduza" para uma versÃ£o reduzida com as informaÃ§Ãµes mais importantes (camada latente) e depois reconstrÃ³i a imagem original a partir dessa versÃ£o compacta.

---

## ğŸ“‚ Estrutura dos Notebooks

### 1. `AE1_CIPAR10_Autoencoder_Orlando.ipynb`
**ğŸ“Œ Tarefa:** ReconstruÃ§Ã£o de imagens do dataset CIFAR-10  
**ğŸ“š Dataset:** [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)  
**ğŸ” Destaques:**
- CodificaÃ§Ã£o de imagens RGB
- ComparaÃ§Ã£o visual entre imagens originais e reconstruÃ­das
- VisualizaÃ§Ã£o do espaÃ§o latente

### 2. `AE2_simples_digitos_Orlando.ipynb`
**ğŸ“Œ Tarefa:** ReconstruÃ§Ã£o de dÃ­gitos manuscritos com arquitetura simples  
**ğŸ“š Dataset:** [MNIST](http://yann.lecun.com/exdb/mnist/)  
**ğŸ” Destaques:**
- Arquitetura leve com codificador e decodificador simÃ©tricos
- VisualizaÃ§Ã£o de reconstruÃ§Ãµes dÃ­gito a dÃ­gito
- AvaliaÃ§Ã£o por erro de reconstruÃ§Ã£o

### 3. `AE3_Deep_Autoencoder_Orlando.ipynb`
**ğŸ“Œ Tarefa:** CompressÃ£o profunda de imagens com autoencoder empilhado  
**ğŸ“š Dataset:** MNIST ou CIFAR-10 (verificado automaticamente no notebook)  
**ğŸ” Destaques:**
- MÃºltiplas camadas densas no encoder e decoder
- CodificaÃ§Ã£o em dimensÃµes muito reduzidas
- AnÃ¡lise da performance na reconstruÃ§Ã£o

---

## ğŸ“ˆ MÃ©tricas e AvaliaÃ§Ã£o

Os autoencoders sÃ£o avaliados por:

- **Erro de reconstruÃ§Ã£o** (ex: MSE)
- **Capacidade de compressÃ£o** (dimensÃ£o latente vs. original)
- **VisualizaÃ§Ã£o das imagens reconstruÃ­das**
- **PercepÃ§Ã£o visual da fidelidade na reconstruÃ§Ã£o**

---

## âš™ï¸ TÃ©cnicas Utilizadas

- Arquiteturas simÃ©tricas Encoder-Decoder
- FunÃ§Ã£o de ativaÃ§Ã£o ReLU e Sigmoid
- Otimizadores como Adam
- VisualizaÃ§Ãµes com Matplotlib
"""
